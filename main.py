"""
Title: Generative Adversarial Network - Video Frame Interpolation (GAN-VFI)
Author: Chris McDonald
Date: February 20, 2024

Description:
This is the main script for the GAN-VFI Project, all other scripts should be called from here.
It includes dataset loading, initilising neural networks and parameters, as well as the training cycle.

Usage:
Run the script using `python main.py` in a terminal.
Make sure python is installed along with the necessary libraries.

Notes:
This file connects multiple scripts to train the GAN on the Vimeo90k dataset:
dataset_statistics.py
dataset_test.py         - Tests if the dataset is properly loaded by plotting each set of frames in a figure.
dataset.py              - Defines the VimeoDataset Class which is used to load the dataset into a variable. All pixels are normalized
                        in the range [-1,1] 
discriminator.py        - Defines the discriminator neural network for the DCGAN implementation. Outputs a single channel probability 
                        of frames being generated by the Generator.
format_time.py
generator.py
"""

import torch                                # PyTorch library - Used for tensors and neural networks in the deep learning model.
from torch.utils.data import DataLoader     # DataLoader class of torch.utils.data module - Used for loading dataset into batches.
from torchvision import transforms          # transforms module of torchvision library - Used for transformation functions applied to images.
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
import random
import torch.optim as optim
import time
import math

# Importing other scripts
from dataset import VimeoDataset
from generator import Generator2D as Generator
from discriminator import Discriminator2D as Discriminator
from format_time import format_duration
from dataset_statistics import dataset_statistics
from dataset_test import dataset_plot

# Attempts to use a GPU if available, otherwise uses the users CPU.
try:
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
except Exception as e:
    print(f"Error during GPU initialization: {e}")
    device = torch.device("cpu")

print(f"Selected device: {device} ({torch.cuda.get_device_name(device.index)}), number of GPU's: {torch.cuda.device_count()}")

# Set seed
manualSeed = 1024
random.seed(manualSeed)
torch.manual_seed(manualSeed)
torch.use_deterministic_algorithms(True)

# Hyperparameters
batch_size = 32
latent_dim = 100
learning_rate = 0.0002
beta1 = 0.5
beta2 = 0.999
num_epochs = 100
worker_threads = 2
colour_channels = 3
lambda_temporal = 0.1
lambda_depth = 0.01

# Global variables
training_batches = int()
training_remainder = int()

train_dataset_mean = [0.3675, 0.3393, 0.3194]
train_dataset_standard_deviation = [0.2111, 0.1870, 0.1717]

dataset_mean = [0.36746213, 0.33925724, 0.31939113]
dataset_standard_deviation = [0.20665579, 0.19933419, 0.19278685]

# Creating DataLoaders for training and testing

train_dataset = VimeoDataset(root_dir='src\\vimeo_septuplet', split='train')
train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=worker_threads)

training_batches = math.ceil(len(train_dataset) / batch_size)
training_remainder = len(train_dataset) % batch_size
#print(f"Dataset has {len(train_dataset)} videos, split into {training_batches} batches of size {batch_size} where the last batch consists of {training_remainder} videos")

test_dataset = VimeoDataset(root_dir='src\\vimeo_septuplet', split='test')
test_data = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=worker_threads)

# Initialize components

N, in_channels, height, width = 8, 3, 512, 512
generator = Generator(latent_dim, in_channels, N).to(device)
discriminator = Discriminator(in_channels, N).to(device)

def initialize_weights(models):
    for model in models:
        for m in model.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):
                nn.init.normal_(m.weight.data, 0.0, 0.02)
            
initialize_weights([generator, discriminator])

# Loss functions
adversarial_loss = nn.BCELoss()
temporal_loss = nn.MSELoss()
depth_loss = nn.MSELoss()

# Optimizers
optimizer_generator = optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, beta2))
optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, beta2))

if __name__ == '__main__':
    print("Inside the main loop")

    dataset_statistics.dataset_options(device, batch_size, training_batches, training_remainder, worker_threads)

    #Testing dataset on a batch
    batch = next(iter(train_data))

    frames, label = batch
    currentframe1 = frames[0][0]
    print(currentframe1.shape)
    print(currentframe1.dtype)

    #Dataset stored in the form Frames, Batch, Channels, Height, Width (F,N,C,H,W)
    print(f'{batch[0][0][0][0].numel()} pixels in each {batch[0][0].size()[2]}x{batch[0][0].size()[3]} RGB image')
    print(f'Pixel values are in range [{torch.min((currentframe1))}, {torch.max((currentframe1))}]')

    plot_dataset = input("Test the dataset by plotting? (y/n)")
    if plot_dataset == 'y':
        dataset_plot(train_data, batch_size)
    else:
        pass

    # Training Loop

for epoch in range(num_epochs):
    for batch_idx, real_data in enumerate(train_data):
        # Train Discriminator
        discriminator.zero_grad()

        # Real data
        real_labels = torch.ones(real_data.size(0), 1)
        output_real = discriminator(real_data)
        loss_real = adversarial_loss(output_real, real_labels)

        # Fake data
        fake_data = generator(torch.randn(real_data.size(0), latent_dim))
        fake_labels = torch.zeros(real_data.size(0), 1)
        output_fake = discriminator(fake_data.detach())
        loss_fake = adversarial_loss(output_fake, fake_labels)

        # Total discriminator loss
        loss_discriminator = loss_real + loss_fake
        loss_discriminator.backward()
        optimizer_discriminator.step()

        # Train Generator
        generator.zero_grad()

        # Generate fake data
        fake_data = generator(torch.randn(real_data.size(0), latent_dim))
        output_generator = discriminator(fake_data)

        # Generator loss
        loss_generator = adversarial_loss(output_generator, real_labels) + \
                         lambda_temporal * temporal_loss(fake_data, real_data) + \
                         lambda_depth * depth_loss(fake_data, real_data)
        
        loss_generator.backward()
        optimizer_generator.step()

        # Print progress
        if batch_idx % 100 == 0:
            print(f'Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{training_batches}], '
                  f'Discriminator Loss: {loss_discriminator.item():.4f}, Generator Loss: {loss_generator.item():.4f}')

# After training, you can use the generator to generate new samples
# generated_samples = generator(torch.randn(num_samples, latent_dim))


